{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"./all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  test.zip  train1  train.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "#plt.rcParams['figure.figsize'] = [16, 10]\n",
    "#plt.rcParams['font.size'] = 16\n",
    "import os\n",
    "#import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import load_files \n",
    "from keras.utils import np_utils\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 total categories.\n",
      "There are 4750 total images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define function to load datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    X = np.array(data['filenames'])\n",
    "    ip_files = np.array([i for i in X])\n",
    "    ip_targets = data['target']\n",
    "    return ip_files, ip_targets\n",
    "\n",
    "# load datasets\n",
    "files, targets = load_dataset('./train1')\n",
    "\n",
    "\n",
    "# load list of plant names\n",
    "# first 15 characters are of path \n",
    "plant_names = [i[9:-1] for i in sorted(glob(\"./train1/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total categories.' % len(plant_names))\n",
    "print('There are %s total images.\\n' % len(files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print (targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black-grass 263 images\n",
      "Charlock 390 images\n",
      "Cleavers 287 images\n",
      "Common Chickweed 611 images\n",
      "Common wheat 221 images\n",
      "Fat Hen 475 images\n",
      "Loose Silky-bent 654 images\n",
      "Maize 221 images\n",
      "Scentless Mayweed 516 images\n",
      "Shepherds Purse 231 images\n",
      "Small-flowered Cranesbill 496 images\n",
      "Sugar beet 385 images\n"
     ]
    }
   ],
   "source": [
    "# How many images per category\n",
    "for name in plant_names:\n",
    "    print('{} {} images'.format(name, len(os.listdir(os.path.join(\"./train1\", name)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(files, targets, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./train1/Fat Hen/45c197012.png'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  test.zip  train1  train.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  test  test.zip  train  train1  train.zip\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir test train \n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['test', 'train']:\n",
    "    for j in plant_names:\n",
    "        path = \"./\"+i+\"/\"+j+\"/\"\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in [(X_train,y_train,\"train\"),(X_test,y_test,\"test\")]:\n",
    "    for counter, old_loc in enumerate(source[0]):\n",
    "        name = old_loc.split(\"/\")[-1]\n",
    "        new_loc = '\"' + \"./\" +source[2] + \"/\" + plant_names[source[1][counter]] + \"/\" + name + '\"'\n",
    "        #print (new_loc)\n",
    "        os.system('cp '+ '\"'+old_loc+'\"' + ' ' + new_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 total categories.\n",
      "Total Images are 4750\n",
      "There are 3800 total training images.\n",
      "\n",
      "There are 950 total testing images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define function to load datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    ip_files = np.array(data['filenames'])\n",
    "    ip_targets = np_utils.to_categorical(np.array(data['target']),12 )\n",
    "    return ip_files, ip_targets\n",
    "\n",
    "# load datasets\n",
    "X_train, y_train = load_dataset('./train')\n",
    "X_valid, y_valid = load_dataset('./test')\n",
    "\n",
    "\n",
    "# load list of plant names\n",
    "# first 15 characters are of pat\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total categories.' % len(plant_names))\n",
    "print (\"Total Images are 4750\")\n",
    "print('There are %s total training images.\\n' % len(X_train))\n",
    "\n",
    "print('There are %s total testing images.\\n' % len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categoriacl',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir, # same directory as training data\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // batch_size,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // batch_size,\n",
    "    epochs = nb_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
